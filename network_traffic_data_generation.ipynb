{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Google Colab, skipping repository clone.\n"
     ]
    }
   ],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# If in Google Colab, clone the repository\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/alexgaarciia/SyntheticTrafficRouting.git\n",
    "    print(\"Repository cloned!\")\n",
    "else:\n",
    "    print(\"Not in Google Colab, skipping repository clone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  import os\n",
    "  os.chdir('SyntheticTrafficRouting/')\n",
    "from functions import combine_first_n_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Traffic Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20514\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset that combines n datasets\n",
    "num_files = 167\n",
    "combined_data = combine_first_n_datasets(folder_path='data11AM', n_files=num_files)\n",
    "combined_data_csv = combined_data.to_csv(\"real_data.csv\", index=False)\n",
    "print(len(combined_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       source  target  demand_value\n",
      "0           0       1      0.000048\n",
      "1           0       2      0.000113\n",
      "2           0       3      0.000028\n",
      "3           0       4      0.000038\n",
      "4           0       5      0.000077\n",
      "...       ...     ...           ...\n",
      "20509      11       6      0.003229\n",
      "20510      11       7      0.020567\n",
      "20511      11       8      0.023564\n",
      "20512      11       9      0.000878\n",
      "20513      11      10      0.007102\n",
      "\n",
      "[20514 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input data for use in a GAN by encoding categorical variables and scaling numerical values\n",
    "# Encode categorical variables\n",
    "le_source = LabelEncoder()\n",
    "le_target = LabelEncoder()\n",
    "combined_data['source'] = le_source.fit_transform(combined_data['source'])\n",
    "combined_data['target'] = le_target.fit_transform(combined_data['target'])\n",
    "\n",
    "# Normalize the demand values to ensure stable gradients\n",
    "scaler = MinMaxScaler()\n",
    "combined_data['demand_value'] = scaler.fit_transform(combined_data[['demand_value']])\n",
    "\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output dimensions\n",
    "input_dim = 10  # noise dimension\n",
    "output_dim = 3  # source, target, demand_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator\n",
    "# The generator takes a noise vector and processes it through 3 fully connected linear layers,\n",
    "# and activation functions (ReLU) are applied to introduce non-linearity\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.source_target_output = nn.Linear(64, output_dim - 1)  # Outputs for source and target\n",
    "        self.demand_output = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()  # Ensures positive demand values\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_output = self.model(x)  # This output has size 64\n",
    "        source_target = self.source_target_output(hidden_output)  # Get source and target\n",
    "        demand = self.demand_output(hidden_output)  # Get demand value\n",
    "        return torch.cat((source_target, demand), dim=1)  # Concatenate source, target, and demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator\n",
    "# The discriminator evaluates real vs generated data. It is made up of fully connected layers\n",
    "# with LeakyReLU activations; the final layer a single probability indicating whether the input\n",
    "# data is real (close to 1) or fake (close to 0)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(output_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = Generator(input_dim, output_dim)\n",
    "discriminator = Discriminator(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers (for updating the weights of the generator and discriminator)\n",
    "lr = 0.0002\n",
    "optim_G = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "optim_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Loss function (for measuring the discrepancy between predicted and true labels)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss_D: 1.2484, Loss_G: 0.9003\n",
      "Epoch 100: Loss_D: 1.3780, Loss_G: 0.6997\n",
      "Epoch 200: Loss_D: 1.3580, Loss_G: 0.6955\n",
      "Epoch 300: Loss_D: 1.3761, Loss_G: 0.7908\n",
      "Epoch 400: Loss_D: 1.3464, Loss_G: 0.7068\n",
      "Epoch 500: Loss_D: 1.4097, Loss_G: 0.6249\n",
      "Epoch 600: Loss_D: 1.3507, Loss_G: 0.7170\n",
      "Epoch 700: Loss_D: 1.3489, Loss_G: 0.7401\n",
      "Epoch 800: Loss_D: 1.3411, Loss_G: 0.7658\n",
      "Epoch 900: Loss_D: 1.4030, Loss_G: 0.6277\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for _ in range(len(combined_data) // batch_size):\n",
    "        # ---------------------\n",
    "        # Train Discriminator\n",
    "        # ---------------------\n",
    "        # Real data\n",
    "        real_data = torch.tensor(combined_data.sample(batch_size).values, dtype=torch.float32)\n",
    "        label_real = torch.full((batch_size, 1), real_label, dtype=torch.float32)  # acts as the ground truth for the discriminator (each element has value 1)\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(batch_size, input_dim)\n",
    "        fake_data = generator(noise)  # The generator creates fakes data\n",
    "        label_fake = torch.full((batch_size, 1), fake_label, dtype=torch.float32)  # similar to 'label_real', but full of 0's (fake data)\n",
    "\n",
    "        # Train on real data\n",
    "        optim_D.zero_grad()\n",
    "        output_real = discriminator(real_data)\n",
    "        loss_real = criterion(output_real, label_real)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Train on fake data\n",
    "        output_fake = discriminator(fake_data.detach())\n",
    "        loss_fake = criterion(output_fake, label_fake)\n",
    "        loss_fake.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Generator\n",
    "        # ---------------------\n",
    "        optim_G.zero_grad()\n",
    "        output = discriminator(fake_data)\n",
    "        loss_G = criterion(output, label_real)  # Aim to fool the discriminator\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "    # Print losses every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss_D: {loss_real + loss_fake:.4f}, Loss_G: {loss_G:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of synthetic samples to generate\n",
    "num_samples = len(combined_data)\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = []\n",
    "for _ in range(num_samples):\n",
    "    # Generate noise\n",
    "    noise = torch.randn(1, input_dim)\n",
    "    generated_sample = generator(noise).detach().numpy()[0]\n",
    "    \n",
    "    # Round and convert source and target to integer indices\n",
    "    source_idx = int(round(generated_sample[0]))\n",
    "    target_idx = int(round(generated_sample[1]))\n",
    "    \n",
    "    # Ensure indices are within the range of the label encoders\n",
    "    source_idx = min(max(source_idx, 0), len(le_source.classes_) - 1)\n",
    "    target_idx = min(max(target_idx, 0), len(le_target.classes_) - 1)\n",
    "\n",
    "    # Map indices back to original categories\n",
    "    synthetic_source = le_source.inverse_transform([source_idx])[0]\n",
    "    synthetic_target = le_target.inverse_transform([target_idx])[0]\n",
    "\n",
    "    # Inverse transform demand_value\n",
    "    synthetic_demand_value = scaler.inverse_transform([[generated_sample[2]]])[0][0]\n",
    "    \n",
    "    # Append the row to the synthetic dataset\n",
    "    synthetic_data.append([synthetic_source, synthetic_target, synthetic_demand_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert synthetic data to a DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=['source', 'target', 'demand_value'])\n",
    "synthetic_df.to_csv('synthetic_data_gan.csv' , index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if there are any repeated values in the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59,\n",
       "        source  target  demand_value\n",
       " 64     STTLng  STTLng     25.624493\n",
       " 453    SNVAng  SNVAng     36.145908\n",
       " 761    ATLAng  ATLAng      0.056667\n",
       " 919    STTLng  STTLng     19.878825\n",
       " 1414   ATLAng  ATLAng      0.029622\n",
       " 1929   LOSAng  LOSAng    117.583500\n",
       " 1995   DNVRng  DNVRng    120.698013\n",
       " 2583   KSCYng  KSCYng     43.922143\n",
       " 2591   KSCYng  KSCYng     48.019180\n",
       " 3490   SNVAng  SNVAng     33.665681\n",
       " 3809   IPLSng  IPLSng     46.600614\n",
       " 4283   HSTNng  HSTNng     49.229470\n",
       " 4390   ATLAng  ATLAng      0.014440\n",
       " 4685   ATLAng  ATLAng      0.101861\n",
       " 4916   ATLAng  ATLAng      0.015702\n",
       " 5004   IPLSng  IPLSng     18.334302\n",
       " 5208   LOSAng  LOSAng     74.205058\n",
       " 5268   ATLAng  ATLAng      0.029818\n",
       " 5498   NYCMng  NYCMng     16.182392\n",
       " 5598   DNVRng  DNVRng     55.897894\n",
       " 5934   NYCMng  NYCMng     21.584031\n",
       " 6987   ATLAng  ATLAng      0.013356\n",
       " 7118   ATLAng  ATLAng      0.047441\n",
       " 7389   DNVRng  DNVRng     87.116650\n",
       " 8043   HSTNng  HSTNng     45.857808\n",
       " 8073   DNVRng  DNVRng     91.232714\n",
       " 8159   KSCYng  KSCYng     79.480624\n",
       " 8717   HSTNng  HSTNng     64.016010\n",
       " 8824   ATLAng  ATLAng      0.038352\n",
       " 9694   ATLAng  ATLAng      0.107378\n",
       " 9920   IPLSng  IPLSng     82.517323\n",
       " 10341  STTLng  STTLng     29.179523\n",
       " 10365  LOSAng  LOSAng     52.440009\n",
       " 10421  HSTNng  HSTNng     60.990039\n",
       " 10648  STTLng  STTLng     27.828520\n",
       " 10650  ATLAng  ATLAng      0.067172\n",
       " 10804  DNVRng  DNVRng    158.534963\n",
       " 11306  LOSAng  LOSAng     59.316024\n",
       " 11420  ATLAng  ATLAng      0.045343\n",
       " 12518  SNVAng  SNVAng     17.656307\n",
       " 12850  DNVRng  DNVRng    118.203410\n",
       " 13019  ATLAng  ATLAng      0.028816\n",
       " 13620  IPLSng  IPLSng     29.162170\n",
       " 14128  KSCYng  KSCYng     22.656384\n",
       " 14274  LOSAng  LOSAng      5.287852\n",
       " 14854  IPLSng  IPLSng     85.170077\n",
       " 15472  IPLSng  IPLSng     43.342578\n",
       " 15571  STTLng  STTLng     46.723725\n",
       " 16012  STTLng  STTLng     28.116662\n",
       " 16378  SNVAng  SNVAng     22.168701\n",
       " 16891  ATLAng  ATLAng      0.025299\n",
       " 17698  NYCMng  NYCMng     75.336958\n",
       " 17763  DNVRng  DNVRng     39.517180\n",
       " 17768  NYCMng  NYCMng     53.886725\n",
       " 18271  IPLSng  IPLSng     45.281080\n",
       " 18694  NYCMng  NYCMng      1.833981\n",
       " 19596  ATLAng  ATLAng      0.042232\n",
       " 20250  ATLAng  ATLAng      0.013914\n",
       " 20441  ATLAng  ATLAng      0.013369)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = 'synthetic_data_gan.csv'\n",
    "synthetic_data = pd.read_csv(file_path)\n",
    "\n",
    "# Check for rows with identical source and target values\n",
    "repeated_rows = synthetic_data[synthetic_data['source'] == synthetic_data['target']]\n",
    "\n",
    "# Display the repeated rows, if any, and count of such rows\n",
    "repeated_rows_count = repeated_rows.shape[0], repeated_rows\n",
    "repeated_rows_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
