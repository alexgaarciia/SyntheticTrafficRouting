{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Traffic Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_network_traffic_data(path):\n",
    "    # Ingest the XML data\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Define the namespace\n",
    "    ns = {'net': 'http://sndlib.zib.de/network'}\n",
    "\n",
    "    # Extract demand information\n",
    "    demands = []\n",
    "    for demand in root.findall('net:demands/net:demand', ns):\n",
    "        source = demand.find('net:source', ns).text\n",
    "        target = demand.find('net:target', ns).text\n",
    "        demand_value = float(demand.find('net:demandValue', ns).text)\n",
    "        demands.append({'source': source, 'target': target, 'demand_value': demand_value})\n",
    "\n",
    "    df = pd.DataFrame(demands)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the input data for use in a Generative Adversarial Network (GAN) by encoding categorical variables\n",
    "# and scaling numerical values\n",
    "data_path = 'input_data/demandMatrix-abilene-zhang-5min-20040910-2325.xml'\n",
    "data = process_network_traffic_data(data_path)\n",
    "print(data)\n",
    "\n",
    "# Encode categorical variables\n",
    "le_source = LabelEncoder()\n",
    "le_target = LabelEncoder()\n",
    "data['source'] = le_source.fit_transform(data['source'])\n",
    "data['target'] = le_target.fit_transform(data['target'])\n",
    "\n",
    "# Normalize the demand values\n",
    "scaler = MinMaxScaler()\n",
    "data['demand_value'] = scaler.fit_transform(data[['demand_value']])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output dimensions\n",
    "input_dim = 10  # noise dimension\n",
    "output_dim = 3  # source, target, demand_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(output_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = Generator(input_dim, output_dim)\n",
    "discriminator = Discriminator(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "lr = 0.0002\n",
    "optim_G = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "optim_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 2000\n",
    "batch_size = 32\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for _ in range(len(data) // batch_size):\n",
    "        # ---------------------\n",
    "        # Train Discriminator\n",
    "        # ---------------------\n",
    "        # Real data\n",
    "        real_data = torch.tensor(data.sample(batch_size).values, dtype=torch.float32)\n",
    "        label_real = torch.full((batch_size, 1), real_label, dtype=torch.float32)\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(batch_size, input_dim)\n",
    "        fake_data = generator(noise)\n",
    "        label_fake = torch.full((batch_size, 1), fake_label, dtype=torch.float32)\n",
    "\n",
    "        # Train on real data\n",
    "        optim_D.zero_grad()\n",
    "        output_real = discriminator(real_data)\n",
    "        loss_real = criterion(output_real, label_real)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Train on fake data\n",
    "        output_fake = discriminator(fake_data.detach())\n",
    "        loss_fake = criterion(output_fake, label_fake)\n",
    "        loss_fake.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Generator\n",
    "        # ---------------------\n",
    "        optim_G.zero_grad()\n",
    "        output = discriminator(fake_data)\n",
    "        loss_G = criterion(output, label_real)  # Aim to fool the discriminator\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "    # Print losses every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss_D: {loss_real + loss_fake:.4f}, Loss_G: {loss_G:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of synthetic samples to generate\n",
    "num_samples = len(data)\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = []\n",
    "for _ in range(num_samples):\n",
    "    # Generate noise\n",
    "    noise = torch.randn(1, input_dim)\n",
    "    generated_sample = generator(noise).detach().numpy()[0]\n",
    "    \n",
    "    # Round and convert source and target to integer indices\n",
    "    source_idx = int(round(generated_sample[0]))\n",
    "    target_idx = int(round(generated_sample[1]))\n",
    "    \n",
    "    # Ensure indices are within the range of the label encoders\n",
    "    source_idx = min(max(source_idx, 0), len(le_source.classes_) - 1)\n",
    "    target_idx = min(max(target_idx, 0), len(le_target.classes_) - 1)\n",
    "\n",
    "    # Map indices back to original categories\n",
    "    synthetic_source = le_source.inverse_transform([source_idx])[0]\n",
    "    synthetic_target = le_target.inverse_transform([target_idx])[0]\n",
    "\n",
    "    # Inverse transform demand_value\n",
    "    synthetic_demand_value = scaler.inverse_transform([[generated_sample[2]]])[0][0]\n",
    "    \n",
    "    # Append the row to the synthetic dataset\n",
    "    synthetic_data.append([synthetic_source, synthetic_target, synthetic_demand_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert synthetic data to a DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=['source', 'target', 'demand_value'])\n",
    "output_csv_path = 'output_data/synthetic_data.csv' \n",
    "synthetic_df.to_csv(output_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
