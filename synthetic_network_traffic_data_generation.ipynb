{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Traffic Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_network_traffic_data(path):\n",
    "    # Ingest the XML data\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Define the namespace\n",
    "    ns = {'net': 'http://sndlib.zib.de/network'}\n",
    "\n",
    "    # Extract demand information\n",
    "    demands = []\n",
    "    for demand in root.findall('net:demands/net:demand', ns):\n",
    "        source = demand.find('net:source', ns).text\n",
    "        target = demand.find('net:target', ns).text\n",
    "        demand_value = float(demand.find('net:demandValue', ns).text)\n",
    "        demands.append({'source': source, 'target': target, 'demand_value': demand_value})\n",
    "\n",
    "    df = pd.DataFrame(demands)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_first_n_datasets(folder_path, n_files=100):\n",
    "    all_data = []  \n",
    "    count = 0  \n",
    "    \n",
    "    # Loop through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            # Process the file and add to the list of dataframes\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            data = process_network_traffic_data(file_path)\n",
    "            all_data.append(data)\n",
    "            count += 1\n",
    "            \n",
    "            # Stop once we have processed n_files\n",
    "            if count >= n_files:\n",
    "                break\n",
    "    \n",
    "    # Concatenate all DataFrames into one large DataFrame\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2582\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset that combines n datasets (there 2000 in the input_data folder)\n",
    "path = 'data'\n",
    "num_files = 20\n",
    "combined_data = combine_first_n_datasets(folder_path=path, n_files=num_files)\n",
    "print(len(combined_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      source  target  demand_value\n",
      "0          0       1      0.000149\n",
      "1          0       2      0.001149\n",
      "2          0       3      0.000070\n",
      "3          0       4      0.000582\n",
      "4          0       5      0.000287\n",
      "...      ...     ...           ...\n",
      "2577      11       6      0.012552\n",
      "2578      11       7      0.072924\n",
      "2579      11       8      0.190076\n",
      "2580      11       9      0.002396\n",
      "2581      11      10      0.049061\n",
      "\n",
      "[2582 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input data for use in a GAN by encoding categorical variables and scaling numerical values\n",
    "# Encode categorical variables\n",
    "le_source = LabelEncoder()\n",
    "le_target = LabelEncoder()\n",
    "combined_data['source'] = le_source.fit_transform(combined_data['source'])\n",
    "combined_data['target'] = le_target.fit_transform(combined_data['target'])\n",
    "\n",
    "# Normalize the demand values\n",
    "scaler = MinMaxScaler()\n",
    "combined_data['demand_value'] = scaler.fit_transform(combined_data[['demand_value']])\n",
    "\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output dimensions\n",
    "input_dim = 10  # noise dimension\n",
    "output_dim = 3  # source, target, demand_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator\n",
    "# The generator takes a noise vector and processes it through 3 fully connected linear layers,\n",
    "# and activation functions (ReLU) are applied to introduce non-linearity\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.source_target_output = nn.Linear(64, output_dim - 1)  # Outputs for source and target\n",
    "        self.demand_output = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()  # Ensures positive demand values\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_output = self.model(x)  # This output has size 64\n",
    "        source_target = self.source_target_output(hidden_output)  # Get source and target\n",
    "        demand = self.demand_output(hidden_output)  # Get demand value\n",
    "        return torch.cat((source_target, demand), dim=1)  # Concatenate source, target, and demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator\n",
    "# The discriminator evaluates real vs generated data. It is made up of fully connected layers\n",
    "# with LeakyReLU activations; the final layer a single probability indicating whether the input\n",
    "# data is real (close to 1) or fake (close to 0)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(output_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = Generator(input_dim, output_dim)\n",
    "discriminator = Discriminator(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers (for updating the weights of the generator and discriminator)\n",
    "lr = 0.0002\n",
    "optim_G = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "optim_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Loss function (for measuring the discrepancy between predicted and true labels)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss_D: 0.9241, Loss_G: 0.6995\n",
      "Epoch 100: Loss_D: 1.3665, Loss_G: 0.6706\n",
      "Epoch 200: Loss_D: 1.3788, Loss_G: 0.6655\n",
      "Epoch 300: Loss_D: 1.3888, Loss_G: 0.7347\n",
      "Epoch 400: Loss_D: 1.3823, Loss_G: 0.6816\n",
      "Epoch 500: Loss_D: 1.3748, Loss_G: 0.6751\n",
      "Epoch 600: Loss_D: 1.3747, Loss_G: 0.7000\n",
      "Epoch 700: Loss_D: 1.3751, Loss_G: 0.6796\n",
      "Epoch 800: Loss_D: 1.3626, Loss_G: 0.7434\n",
      "Epoch 900: Loss_D: 1.3526, Loss_G: 0.7098\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for _ in range(len(combined_data) // batch_size):\n",
    "        # ---------------------\n",
    "        # Train Discriminator\n",
    "        # ---------------------\n",
    "        # Real data\n",
    "        real_data = torch.tensor(combined_data.sample(batch_size).values, dtype=torch.float32)\n",
    "        label_real = torch.full((batch_size, 1), real_label, dtype=torch.float32)  # acts as the ground truth for the discriminator (each element has value 1)\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(batch_size, input_dim)\n",
    "        fake_data = generator(noise)  # The generator creates fakes data\n",
    "        label_fake = torch.full((batch_size, 1), fake_label, dtype=torch.float32)  # similar to 'label_real', but full of 0's (fake data)\n",
    "\n",
    "        # Train on real data\n",
    "        optim_D.zero_grad()\n",
    "        output_real = discriminator(real_data)\n",
    "        loss_real = criterion(output_real, label_real)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Train on fake data\n",
    "        output_fake = discriminator(fake_data.detach())\n",
    "        loss_fake = criterion(output_fake, label_fake)\n",
    "        loss_fake.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Generator\n",
    "        # ---------------------\n",
    "        optim_G.zero_grad()\n",
    "        output = discriminator(fake_data)\n",
    "        loss_G = criterion(output, label_real)  # Aim to fool the discriminator\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "    # Print losses every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss_D: {loss_real + loss_fake:.4f}, Loss_G: {loss_G:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of synthetic samples to generate\n",
    "num_samples = len(combined_data)\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = []\n",
    "for _ in range(num_samples):\n",
    "    # Generate noise\n",
    "    noise = torch.randn(1, input_dim)\n",
    "    generated_sample = generator(noise).detach().numpy()[0]\n",
    "    \n",
    "    # Round and convert source and target to integer indices\n",
    "    source_idx = int(round(generated_sample[0]))\n",
    "    target_idx = int(round(generated_sample[1]))\n",
    "    \n",
    "    # Ensure indices are within the range of the label encoders\n",
    "    source_idx = min(max(source_idx, 0), len(le_source.classes_) - 1)\n",
    "    target_idx = min(max(target_idx, 0), len(le_target.classes_) - 1)\n",
    "\n",
    "    # Enforce the constraint that source and target should not be the same\n",
    "    while source_idx == target_idx:\n",
    "        noise = torch.randn(1, input_dim)  # Regenerate noise\n",
    "        generated_sample = generator(noise).detach().numpy()[0]\n",
    "        source_idx = int(round(generated_sample[0]))\n",
    "        target_idx = int(round(generated_sample[1]))\n",
    "        \n",
    "        # Ensure indices are within the range again after regeneration\n",
    "        source_idx = min(max(source_idx, 0), len(le_source.classes_) - 1)\n",
    "        target_idx = min(max(target_idx, 0), len(le_target.classes_) - 1)\n",
    "\n",
    "    # Map indices back to original categories\n",
    "    synthetic_source = le_source.inverse_transform([source_idx])[0]\n",
    "    synthetic_target = le_target.inverse_transform([target_idx])[0]\n",
    "\n",
    "    # Inverse transform demand_value\n",
    "    synthetic_demand_value = scaler.inverse_transform([[generated_sample[2]]])[0][0]\n",
    "    \n",
    "    # Append the row to the synthetic dataset\n",
    "    synthetic_data.append([synthetic_source, synthetic_target, synthetic_demand_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert synthetic data to a DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=['source', 'target', 'demand_value'])\n",
    "output_csv_path = 'synthetic_data.csv' \n",
    "synthetic_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if there are any repeated values in the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " Empty DataFrame\n",
       " Columns: [source, target, demand_value]\n",
       " Index: [])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = 'synthetic_data.csv'\n",
    "synthetic_data = pd.read_csv(file_path)\n",
    "\n",
    "# Check for rows with identical source and target values\n",
    "repeated_rows = synthetic_data[synthetic_data['source'] == synthetic_data['target']]\n",
    "\n",
    "# Display the repeated rows, if any, and count of such rows\n",
    "repeated_rows_count = repeated_rows.shape[0], repeated_rows\n",
    "repeated_rows_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
