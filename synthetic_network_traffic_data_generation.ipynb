{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Traffic Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_network_traffic_data(path):\n",
    "    # Ingest the XML data\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Define the namespace\n",
    "    ns = {'net': 'http://sndlib.zib.de/network'}\n",
    "\n",
    "    # Extract demand information\n",
    "    demands = []\n",
    "    for demand in root.findall('net:demands/net:demand', ns):\n",
    "        source = demand.find('net:source', ns).text\n",
    "        target = demand.find('net:target', ns).text\n",
    "        demand_value = float(demand.find('net:demandValue', ns).text)\n",
    "        demands.append({'source': source, 'target': target, 'demand_value': demand_value})\n",
    "\n",
    "    df = pd.DataFrame(demands)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     source  target  demand_value\n",
      "0    ATLAM5  ATLAng      0.396835\n",
      "1    ATLAM5  CHINng      3.139035\n",
      "2    ATLAM5  DNVRng      0.475848\n",
      "3    ATLAM5  HSTNng      0.026667\n",
      "4    ATLAM5  IPLSng      1.406133\n",
      "..      ...     ...           ...\n",
      "126  WASHng  KSCYng     39.996867\n",
      "127  WASHng  LOSAng    139.748747\n",
      "128  WASHng  NYCMng    210.302072\n",
      "129  WASHng  SNVAng     11.043411\n",
      "130  WASHng  STTLng     65.051659\n",
      "\n",
      "[131 rows x 3 columns]\n",
      "     source  target  demand_value\n",
      "0         0       1      0.001760\n",
      "1         0       2      0.014801\n",
      "2         0       3      0.002136\n",
      "3         0       4      0.000000\n",
      "4         0       5      0.006560\n",
      "..      ...     ...           ...\n",
      "126      11       6      0.190085\n",
      "127      11       7      0.664472\n",
      "128      11       8      1.000000\n",
      "129      11       9      0.052392\n",
      "130      11      10      0.309237\n",
      "\n",
      "[131 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input data for use in a GAN by encoding categorical variables and scaling numerical values\n",
    "data_path = 'input_data/demandMatrix-abilene-zhang-5min-20040910-2225.xml'\n",
    "data = process_network_traffic_data(data_path)\n",
    "print(data)\n",
    "\n",
    "# Encode categorical variables\n",
    "le_source = LabelEncoder()\n",
    "le_target = LabelEncoder()\n",
    "data['source'] = le_source.fit_transform(data['source'])\n",
    "data['target'] = le_target.fit_transform(data['target'])\n",
    "\n",
    "# Normalize the demand values\n",
    "scaler = MinMaxScaler()\n",
    "data['demand_value'] = scaler.fit_transform(data[['demand_value']])\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output dimensions\n",
    "input_dim = 10  # noise dimension\n",
    "output_dim = 3  # source, target, demand_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator\n",
    "# The generator takes a noise vector and processes it through 3 fully connected linear layers,\n",
    "# and activation functions (ReLU) are applied to introduce non-linearity\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.source_target_output = nn.Linear(64, output_dim - 1)  # Outputs for source and target\n",
    "        self.demand_output = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()  # Ensures positive demand values\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_output = self.model(x)  # This output has size 64\n",
    "        source_target = self.source_target_output(hidden_output)  # Get source and target\n",
    "        demand = self.demand_output(hidden_output)  # Get demand value\n",
    "        return torch.cat((source_target, demand), dim=1)  # Concatenate source, target, and demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator\n",
    "# The discriminator evaluates real vs generated data. It is made up of fully connected layers\n",
    "# with LeakyReLU activations; the final layer a single probability indicating whether the input\n",
    "# data is real (close to 1) or fake (close to 0)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(output_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = Generator(input_dim, output_dim)\n",
    "discriminator = Discriminator(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers (for updating the weights of the generator and discriminator)\n",
    "lr = 0.0002\n",
    "optim_G = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "optim_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Loss function (for measuring the discrepancy between predicted and true labels)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 2000\n",
    "batch_size = 32\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for _ in range(len(data) // batch_size):\n",
    "        # ---------------------\n",
    "        # Train Discriminator\n",
    "        # ---------------------\n",
    "        # Real data\n",
    "        real_data = torch.tensor(data.sample(batch_size).values, dtype=torch.float32)\n",
    "        label_real = torch.full((batch_size, 1), real_label, dtype=torch.float32)  # acts as the ground truth for the discriminator (each element has value 1)\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(batch_size, input_dim)\n",
    "        fake_data = generator(noise)  # The generator creates fakes data\n",
    "        label_fake = torch.full((batch_size, 1), fake_label, dtype=torch.float32)  # similar to 'label_real', but full of 0's (fake data)\n",
    "\n",
    "        # Train on real data\n",
    "        optim_D.zero_grad()\n",
    "        output_real = discriminator(real_data)\n",
    "        loss_real = criterion(output_real, label_real)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Train on fake data\n",
    "        output_fake = discriminator(fake_data.detach())\n",
    "        loss_fake = criterion(output_fake, label_fake)\n",
    "        loss_fake.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Generator\n",
    "        # ---------------------\n",
    "        optim_G.zero_grad()\n",
    "        output = discriminator(fake_data)\n",
    "        loss_G = criterion(output, label_real)  # Aim to fool the discriminator\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "    # Print losses every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss_D: {loss_real + loss_fake:.4f}, Loss_G: {loss_G:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of synthetic samples to generate\n",
    "num_samples = len(data)\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = []\n",
    "for _ in range(num_samples):\n",
    "    # Generate noise\n",
    "    noise = torch.randn(1, input_dim)\n",
    "    generated_sample = generator(noise).detach().numpy()[0]\n",
    "    \n",
    "    # Round and convert source and target to integer indices\n",
    "    source_idx = int(round(generated_sample[0]))\n",
    "    target_idx = int(round(generated_sample[1]))\n",
    "    \n",
    "    # Ensure indices are within the range of the label encoders\n",
    "    source_idx = min(max(source_idx, 0), len(le_source.classes_) - 1)\n",
    "    target_idx = min(max(target_idx, 0), len(le_target.classes_) - 1)\n",
    "\n",
    "    # Map indices back to original categories\n",
    "    synthetic_source = le_source.inverse_transform([source_idx])[0]\n",
    "    synthetic_target = le_target.inverse_transform([target_idx])[0]\n",
    "\n",
    "    # Inverse transform demand_value\n",
    "    synthetic_demand_value = scaler.inverse_transform([[generated_sample[2]]])[0][0]\n",
    "    \n",
    "    # Append the row to the synthetic dataset\n",
    "    synthetic_data.append([synthetic_source, synthetic_target, synthetic_demand_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert synthetic data to a DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=['source', 'target', 'demand_value'])\n",
    "output_csv_path = 'output_data/synthetic_data.csv' \n",
    "synthetic_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if there are any repeated values in the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,\n",
       "      source  target  demand_value\n",
       " 3    STTLng  STTLng     45.007594\n",
       " 5    DNVRng  DNVRng     70.265431\n",
       " 6    IPLSng  IPLSng     54.982571\n",
       " 9    WASHng  WASHng     53.779053\n",
       " 19   ATLAM5  ATLAM5      1.542439\n",
       " 33   HSTNng  HSTNng     20.217529\n",
       " 35   WASHng  WASHng     48.909946\n",
       " 62   LOSAng  LOSAng     65.500599\n",
       " 87   HSTNng  HSTNng     10.028546\n",
       " 88   HSTNng  HSTNng     75.168888\n",
       " 113  LOSAng  LOSAng     53.296147\n",
       " 122  WASHng  WASHng     50.659047)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = 'output_data/synthetic_data.csv'\n",
    "synthetic_data = pd.read_csv(file_path)\n",
    "\n",
    "# Check for rows with identical source and target values\n",
    "repeated_rows = synthetic_data[synthetic_data['source'] == synthetic_data['target']]\n",
    "\n",
    "# Display the repeated rows, if any, and count of such rows\n",
    "repeated_rows_count = repeated_rows.shape[0], repeated_rows\n",
    "repeated_rows_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
