{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Traffic Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_network_traffic_data(path):\n",
    "    # Ingest the XML data\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Define the namespace\n",
    "    ns = {'net': 'http://sndlib.zib.de/network'}\n",
    "\n",
    "    # Extract demand information\n",
    "    demands = []\n",
    "    for demand in root.findall('net:demands/net:demand', ns):\n",
    "        source = demand.find('net:source', ns).text\n",
    "        target = demand.find('net:target', ns).text\n",
    "        demand_value = float(demand.find('net:demandValue', ns).text)\n",
    "        demands.append({'source': source, 'target': target, 'demand_value': demand_value})\n",
    "\n",
    "    df = pd.DataFrame(demands)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     source  target  demand_value\n",
      "0    ATLAM5  ATLAng      0.709813\n",
      "1    ATLAM5  CHINng      0.809493\n",
      "2    ATLAM5  DNVRng      0.251613\n",
      "3    ATLAM5  HSTNng      0.245597\n",
      "4    ATLAM5  IPLSng      1.273816\n",
      "..      ...     ...           ...\n",
      "124  WASHng  KSCYng     26.333123\n",
      "125  WASHng  LOSAng    115.699184\n",
      "126  WASHng  NYCMng    230.741349\n",
      "127  WASHng  SNVAng     11.052075\n",
      "128  WASHng  STTLng     66.994456\n",
      "\n",
      "[129 rows x 3 columns]\n",
      "     source  target  demand_value\n",
      "0         0       1      0.002938\n",
      "1         0       2      0.003370\n",
      "2         0       3      0.000952\n",
      "3         0       4      0.000926\n",
      "4         0       5      0.005383\n",
      "..      ...     ...           ...\n",
      "124      11       6      0.114001\n",
      "125      11       7      0.501355\n",
      "126      11       8      1.000000\n",
      "127      11       9      0.047766\n",
      "128      11      10      0.290246\n",
      "\n",
      "[129 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input data for use in a GAN by encoding categorical variables and scaling numerical values\n",
    "data_path = 'input_data/demandMatrix-abilene-zhang-5min-20040910-2325.xml'\n",
    "data = process_network_traffic_data(data_path)\n",
    "print(data)\n",
    "\n",
    "# Encode categorical variables\n",
    "le_source = LabelEncoder()\n",
    "le_target = LabelEncoder()\n",
    "data['source'] = le_source.fit_transform(data['source'])\n",
    "data['target'] = le_target.fit_transform(data['target'])\n",
    "\n",
    "# Normalize the demand values\n",
    "scaler = MinMaxScaler()\n",
    "data['demand_value'] = scaler.fit_transform(data[['demand_value']])\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output dimensions\n",
    "input_dim = 10  # noise dimension\n",
    "output_dim = 3  # source, target, demand_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator\n",
    "# The generator takes a noise vector and processes it through 3 fully connected linear layers,\n",
    "# and activation functions (ReLU) are applied to introduce non-linearity\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.source_target_output = nn.Linear(64, output_dim - 1)  # Outputs for source and target\n",
    "        self.demand_output = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()  # Ensures positive demand values\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_output = self.model(x)  # This output has size 64\n",
    "        source_target = self.source_target_output(hidden_output)  # Get source and target\n",
    "        demand = self.demand_output(hidden_output)  # Get demand value\n",
    "        return torch.cat((source_target, demand), dim=1)  # Concatenate source, target, and demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator\n",
    "# The discriminator evaluates real vs generated data. It is made up of fully connected layers\n",
    "# with LeakyReLU activations; the final layer a single probability indicating whether the input\n",
    "# data is real (close to 1) or fake (close to 0)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(output_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = Generator(input_dim, output_dim)\n",
    "discriminator = Discriminator(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers (for updating the weights of the generator and discriminator)\n",
    "lr = 0.0002\n",
    "optim_G = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "optim_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Loss function (for measuring the discrepancy between predicted and true labels)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 2000\n",
    "batch_size = 32\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss_D: 1.3657, Loss_G: 0.7498\n",
      "Epoch 100: Loss_D: 0.9200, Loss_G: 1.1510\n",
      "Epoch 200: Loss_D: 0.7852, Loss_G: 1.1715\n",
      "Epoch 300: Loss_D: 1.5400, Loss_G: 0.8757\n",
      "Epoch 400: Loss_D: 0.8381, Loss_G: 1.1230\n",
      "Epoch 500: Loss_D: 0.6464, Loss_G: 1.3238\n",
      "Epoch 600: Loss_D: 1.5715, Loss_G: 0.7616\n",
      "Epoch 700: Loss_D: 1.4728, Loss_G: 0.7522\n",
      "Epoch 800: Loss_D: 1.4310, Loss_G: 0.6405\n",
      "Epoch 900: Loss_D: 1.3430, Loss_G: 0.7489\n",
      "Epoch 1000: Loss_D: 1.3550, Loss_G: 0.7033\n",
      "Epoch 1100: Loss_D: 1.4290, Loss_G: 0.5932\n",
      "Epoch 1200: Loss_D: 1.4007, Loss_G: 0.6746\n",
      "Epoch 1300: Loss_D: 1.3794, Loss_G: 0.7255\n",
      "Epoch 1400: Loss_D: 1.4003, Loss_G: 0.6780\n",
      "Epoch 1500: Loss_D: 1.4208, Loss_G: 0.6552\n",
      "Epoch 1600: Loss_D: 1.3860, Loss_G: 0.6933\n",
      "Epoch 1700: Loss_D: 1.3938, Loss_G: 0.6622\n",
      "Epoch 1800: Loss_D: 1.4589, Loss_G: 0.8237\n",
      "Epoch 1900: Loss_D: 1.3919, Loss_G: 0.7056\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for _ in range(len(data) // batch_size):\n",
    "        # ---------------------\n",
    "        # Train Discriminator\n",
    "        # ---------------------\n",
    "        # Real data\n",
    "        real_data = torch.tensor(data.sample(batch_size).values, dtype=torch.float32)\n",
    "        label_real = torch.full((batch_size, 1), real_label, dtype=torch.float32)  # acts as the ground truth for the discriminator (each element has value 1)\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(batch_size, input_dim)\n",
    "        fake_data = generator(noise)  # The generator creates fakes data\n",
    "        label_fake = torch.full((batch_size, 1), fake_label, dtype=torch.float32)  # similar to 'label_real', but full of 0's (fake data)\n",
    "\n",
    "        # Train on real data\n",
    "        optim_D.zero_grad()\n",
    "        output_real = discriminator(real_data)\n",
    "        loss_real = criterion(output_real, label_real)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Train on fake data\n",
    "        output_fake = discriminator(fake_data.detach())\n",
    "        loss_fake = criterion(output_fake, label_fake)\n",
    "        loss_fake.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Generator\n",
    "        # ---------------------\n",
    "        optim_G.zero_grad()\n",
    "        output = discriminator(fake_data)\n",
    "        loss_G = criterion(output, label_real)  # Aim to fool the discriminator\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "    # Print losses every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss_D: {loss_real + loss_fake:.4f}, Loss_G: {loss_G:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of synthetic samples to generate\n",
    "num_samples = len(data)\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = []\n",
    "for _ in range(num_samples):\n",
    "    # Generate noise\n",
    "    noise = torch.randn(1, input_dim)\n",
    "    generated_sample = generator(noise).detach().numpy()[0]\n",
    "    \n",
    "    # Round and convert source and target to integer indices\n",
    "    source_idx = int(round(generated_sample[0]))\n",
    "    target_idx = int(round(generated_sample[1]))\n",
    "    \n",
    "    # Ensure indices are within the range of the label encoders\n",
    "    source_idx = min(max(source_idx, 0), len(le_source.classes_) - 1)\n",
    "    target_idx = min(max(target_idx, 0), len(le_target.classes_) - 1)\n",
    "\n",
    "    # Map indices back to original categories\n",
    "    synthetic_source = le_source.inverse_transform([source_idx])[0]\n",
    "    synthetic_target = le_target.inverse_transform([target_idx])[0]\n",
    "\n",
    "    # Inverse transform demand_value\n",
    "    synthetic_demand_value = scaler.inverse_transform([[generated_sample[2]]])[0][0]\n",
    "    \n",
    "    # Append the row to the synthetic dataset\n",
    "    synthetic_data.append([synthetic_source, synthetic_target, synthetic_demand_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert synthetic data to a DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=['source', 'target', 'demand_value'])\n",
    "output_csv_path = 'output_data/synthetic_data.csv' \n",
    "synthetic_df.to_csv(output_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
