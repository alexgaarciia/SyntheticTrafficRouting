{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Traffic Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_network_traffic_data(path):\n",
    "    # Ingest the XML data\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Define the namespace\n",
    "    ns = {'net': 'http://sndlib.zib.de/network'}\n",
    "\n",
    "    # Extract demand information\n",
    "    demands = []\n",
    "    for demand in root.findall('net:demands/net:demand', ns):\n",
    "        source = demand.find('net:source', ns).text\n",
    "        target = demand.find('net:target', ns).text\n",
    "        demand_value = float(demand.find('net:demandValue', ns).text)\n",
    "        demands.append({'source': source, 'target': target, 'demand_value': demand_value})\n",
    "\n",
    "    df = pd.DataFrame(demands)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the input data for use in a Generative Adversarial Network (GAN) by encoding categorical variables\n",
    "# and scaling numerical values\n",
    "data_path = 'data/demandMatrix-abilene-zhang-5min-20040910-2325.xml'\n",
    "data = process_network_traffic_data(data_path)\n",
    "\n",
    "# Encode categorical variables\n",
    "le_source = LabelEncoder()\n",
    "le_target = LabelEncoder()\n",
    "data['source'] = le_source.fit_transform(data['source'])\n",
    "data['target'] = le_target.fit_transform(data['target'])\n",
    "\n",
    "# Normalize the demand values\n",
    "scaler = MinMaxScaler()\n",
    "data['demand_value'] = scaler.fit_transform(data[['demand_value']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output dimensions\n",
    "input_dim = 3  # noise dimension\n",
    "output_dim = 3  # source, target, demand_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.Tanh() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(output_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Outputs probability of being real\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = Generator(input_dim, output_dim)\n",
    "discriminator = Discriminator(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "lr = 0.0002\n",
    "optim_G = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "optim_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5000\n",
    "batch_size = 32\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss_D: 0.9886, Loss_G: 0.6879\n",
      "Epoch 100: Loss_D: 0.2918, Loss_G: 2.0587\n",
      "Epoch 200: Loss_D: 0.1180, Loss_G: 3.5142\n",
      "Epoch 300: Loss_D: 0.0221, Loss_G: 3.8590\n",
      "Epoch 400: Loss_D: 0.0798, Loss_G: 4.3452\n",
      "Epoch 500: Loss_D: 0.0858, Loss_G: 4.2078\n",
      "Epoch 600: Loss_D: 0.0530, Loss_G: 4.8280\n",
      "Epoch 700: Loss_D: 0.0796, Loss_G: 3.4740\n",
      "Epoch 800: Loss_D: 0.1194, Loss_G: 4.6378\n",
      "Epoch 900: Loss_D: 0.0070, Loss_G: 5.0430\n",
      "Epoch 1000: Loss_D: 0.0051, Loss_G: 5.2910\n",
      "Epoch 1100: Loss_D: 0.0086, Loss_G: 4.8148\n",
      "Epoch 1200: Loss_D: 0.0116, Loss_G: 4.5921\n",
      "Epoch 1300: Loss_D: 0.1567, Loss_G: 4.6870\n",
      "Epoch 1400: Loss_D: 0.0082, Loss_G: 4.8249\n",
      "Epoch 1500: Loss_D: 0.0095, Loss_G: 4.6630\n",
      "Epoch 1600: Loss_D: 0.0070, Loss_G: 5.0531\n",
      "Epoch 1700: Loss_D: 0.0058, Loss_G: 5.2314\n",
      "Epoch 1800: Loss_D: 0.0071, Loss_G: 4.9628\n",
      "Epoch 1900: Loss_D: 0.0088, Loss_G: 4.7540\n",
      "Epoch 2000: Loss_D: 0.0044, Loss_G: 5.5011\n",
      "Epoch 2100: Loss_D: 0.0077, Loss_G: 4.8899\n",
      "Epoch 2200: Loss_D: 0.0044, Loss_G: 5.4881\n",
      "Epoch 2300: Loss_D: 0.0096, Loss_G: 4.6595\n",
      "Epoch 2400: Loss_D: 0.0076, Loss_G: 4.8859\n",
      "Epoch 2500: Loss_D: 0.1639, Loss_G: 5.0509\n",
      "Epoch 2600: Loss_D: 0.0079, Loss_G: 4.8890\n",
      "Epoch 2700: Loss_D: 0.0102, Loss_G: 4.5866\n",
      "Epoch 2800: Loss_D: 0.1503, Loss_G: 4.4095\n",
      "Epoch 2900: Loss_D: 0.0126, Loss_G: 4.3869\n",
      "Epoch 3000: Loss_D: 0.0083, Loss_G: 4.7930\n",
      "Epoch 3100: Loss_D: 0.0082, Loss_G: 4.8304\n",
      "Epoch 3200: Loss_D: 0.0078, Loss_G: 4.8820\n",
      "Epoch 3300: Loss_D: 0.0064, Loss_G: 5.0654\n",
      "Epoch 3400: Loss_D: 0.0045, Loss_G: 5.4097\n",
      "Epoch 3500: Loss_D: 0.0062, Loss_G: 5.0872\n",
      "Epoch 3600: Loss_D: 0.0058, Loss_G: 5.1248\n",
      "Epoch 3700: Loss_D: 0.0107, Loss_G: 4.5467\n",
      "Epoch 3800: Loss_D: 0.0077, Loss_G: 4.8343\n",
      "Epoch 3900: Loss_D: 0.0073, Loss_G: 4.9242\n",
      "Epoch 4000: Loss_D: 0.0050, Loss_G: 5.3194\n",
      "Epoch 4100: Loss_D: 0.1634, Loss_G: 5.0209\n",
      "Epoch 4200: Loss_D: 0.0102, Loss_G: 4.5708\n",
      "Epoch 4300: Loss_D: 0.0083, Loss_G: 4.7932\n",
      "Epoch 4400: Loss_D: 0.0082, Loss_G: 4.7840\n",
      "Epoch 4500: Loss_D: 0.1589, Loss_G: 4.7874\n",
      "Epoch 4600: Loss_D: 0.0042, Loss_G: 5.4613\n",
      "Epoch 4700: Loss_D: 0.1457, Loss_G: 4.1291\n",
      "Epoch 4800: Loss_D: 0.0089, Loss_G: 4.6936\n",
      "Epoch 4900: Loss_D: 0.0080, Loss_G: 4.8438\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for _ in range(len(data) // batch_size):\n",
    "        # ---------------------\n",
    "        # Train Discriminator\n",
    "        # ---------------------\n",
    "        # Real data\n",
    "        real_data = torch.tensor(data.sample(batch_size).values, dtype=torch.float32)\n",
    "        label_real = torch.full((batch_size, 1), real_label, dtype=torch.float32)\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(batch_size, input_dim)\n",
    "        fake_data = generator(noise)\n",
    "        label_fake = torch.full((batch_size, 1), fake_label, dtype=torch.float32)\n",
    "\n",
    "        # Train on real data\n",
    "        optim_D.zero_grad()\n",
    "        output_real = discriminator(real_data)\n",
    "        loss_real = criterion(output_real, label_real)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Train on fake data\n",
    "        output_fake = discriminator(fake_data.detach())\n",
    "        loss_fake = criterion(output_fake, label_fake)\n",
    "        loss_fake.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Generator\n",
    "        # ---------------------\n",
    "        optim_G.zero_grad()\n",
    "        output = discriminator(fake_data)\n",
    "        loss_G = criterion(output, label_real)  # Aim to fool the discriminator\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "    # Print losses every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss_D: {loss_real + loss_fake:.4f}, Loss_G: {loss_G:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of synthetic samples to generate\n",
    "num_samples = len(data)\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = []\n",
    "for _ in range(num_samples):\n",
    "    # Generate noise\n",
    "    noise = torch.randn(1, input_dim)\n",
    "    generated_sample = generator(noise).detach().numpy()[0]\n",
    "\n",
    "    # Scale generated values to the range of indices for source and target\n",
    "    source_idx = int(((generated_sample[0] + 1) / 2) * (len(le_source.classes_) - 1))\n",
    "    target_idx = int(((generated_sample[1] + 1) / 2) * (len(le_target.classes_) - 1))\n",
    "\n",
    "    # Map indices back to original categories\n",
    "    synthetic_source = le_source.inverse_transform([source_idx])[0]\n",
    "    synthetic_target = le_target.inverse_transform([target_idx])[0]\n",
    "\n",
    "    # Scale and add noise to demand_value for variety\n",
    "    synthetic_demand_value = scaler.inverse_transform([[generated_sample[2]]])[0][0]\n",
    "    synthetic_demand_value += (0.05 * synthetic_demand_value * np.random.randn())\n",
    "\n",
    "    # Append the row to the synthetic dataset\n",
    "    synthetic_data.append([synthetic_source, synthetic_target, synthetic_demand_value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert synthetic data to a DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=['source', 'target', 'demand_value'])\n",
    "output_csv_path = 'data/synthetic_data.csv'  # Make sure the 'data' directory exists\n",
    "synthetic_df.to_csv(output_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
